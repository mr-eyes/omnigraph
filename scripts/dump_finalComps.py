"""
This script is used under the project "Omnigraph"

Input:
    1. sqlite DB File
    2. PairsCount TSV file generated by the executable: single_primaryPartitioning.
    3. cutoff threshold, filter out edges with weight < cutoff.

Output:
    1. Directory of fasta files, each file contains a final component sequences.
    2. TSV with col1:finalCompID col2:originalComponentsIDs


Run:
python dump_finalComps.py <db_file> <pairsCountFile> <originalComponentsCSV> <no_cores> <optional: cutoff (default: 1)>
"""

import sqlite3
import sys
from collections import defaultdict
import os
from tqdm import tqdm
import multiprocessing


class ConnectedComponents:

    isolated_source = list()
    isolated_target = list()

    def __init__(self, min_count=1):
        self.__source = list()
        self.__target = list()
        self.final_components = dict()
        self.number_of_components = int()
        self.THRESHOLD = min_count

    def add_edge(self, source_node, target_node, pair_count):
        if pair_count >= self.THRESHOLD:
            self.__source.append(source_node)
            self.__target.append(target_node)
        else:
            self.isolated_source.append(source_node)
            self.isolated_target.append(target_node)

    def construct(self):
        __leaders = defaultdict(lambda: None)
        __groups = defaultdict(set)

        def __find(x):
            l = __leaders[x]
            if l is not None:
                l = __find(l)
                __leaders[x] = l
                return l
            return x

        def __union(x, y):
            lx, ly = __find(x), __find(y)
            if lx != ly:
                __leaders[lx] = ly

        for i in range(len(self.__source)):
            __union(self.__source[i], self.__target[i])

        for x in __leaders:
            __groups[__find(x)].add(x)

        _last_key = 0
        for component_id, (_k, _v) in enumerate(__groups.items(), start=1):
            self.final_components[component_id] = _v
            _last_key = component_id

        # Disabled until further invesigation
        # Adding isolated components
        # for i in range(len(self.isolated_source)):
        #     _last_key += 1
        #     self.final_components[_last_key] = [self.isolated_source[i],self.isolated_target[i]]


        self.number_of_components = len(self.final_components)

    def get_components_dict(self):
        return self.final_components

    def dump_to_tsv(self, file_name):
        with open(file_name, 'w') as tsvWriter:
            for compID, nodes in self.final_components.items():
                nodes = ','.join(map(str, nodes))
                tsvWriter.write(f"{compID}\t{nodes}\n")

    def __del__(self):
        del self


def get_nodes_sizes(components_file_path):
    """
    Return the size of each original component so we can apply threshold on weight.
    :param components_file_path:
    :return: node_to_size
    """
    node_to_size = dict()
    with open(components_file_path, 'r') as compsReader:
        for _line in compsReader:
            _line = list(map(int, _line.strip().split(',')))
            _compID = int(_line[0])
            node_to_size[_compID] = len(_line) - 1

    return node_to_size


if len(sys.argv) < 4:
    raise ValueError("run: python dump_finalComps.py <db_file> <pairsCountFile> <originalCompsCSV> <no_cores> <cutoff>")

cutoff_threshold = 1
sqlite_db_path = sys.argv[1]
pairsCountFile = sys.argv[2]
originalComponentsCSV = sys.argv[3]
no_cores = int(sys.argv[4])

if len(sys.argv) == 6:
    cutoff_threshold = int(sys.argv[5])

"""
1. Parse the pairsCount to edges
"""

edges = []
with open(pairsCountFile, 'r') as pairsCountReader:
    next(pairsCountReader)  # skip header
    for line in pairsCountReader:
        edges.append(tuple(map(int, line.strip().split())))

"""
2. Construct final components
"""

components = ConnectedComponents(min_count=cutoff_threshold)
for edge in edges:
    components.add_edge(*edge)

components.construct()
number_of_final_components = components.number_of_components

print(f"Number of connected comps: {number_of_final_components}")


""" 2.1 Adding Isolated Components """


numberOfOriginalComponents = 0
with open(originalComponentsCSV) as origCompReader:
    for line in origCompReader:
        numberOfOriginalComponents += 1

final_components = components.get_components_dict()
gathered_originalComponents = set()
for k, v in final_components.items():
    for origComp in v:
        gathered_originalComponents.add(origComp)

isoloatedComponents_set = set(range(1, numberOfOriginalComponents+1)) - gathered_originalComponents

# Adding the isolated components to the final components
last_key = max(final_components.keys()) + 1
for i, isolatedComp in enumerate(isoloatedComponents_set, start=last_key):
    final_components[i] = [isolatedComp]



"""
3. Multithreaded dumping the partitions to fasta files
"""

"""

1. One of them is 0
2. Two queries:
    - Query1: search by R1 matching to any original component in the final component or unmatched (0) where R2 matching any original component in the same final component
    - Query2: R1 matched to any original component in the final component and R2 is 0

"""


def perform_writing(params):
    def finalComp_to_pairs(original_comps):
        for _comp1 in original_comps:
            for _comp2 in original_comps:
                yield _comp1, _comp2
        for _comp2 in original_comps:
            yield 0, _comp2
        for _comp1 in original_comps:
            yield _comp1, 0

    file_path, _finalCompID, _originalComps = params
    conn = sqlite3.connect(sqlite_db_path)
    _originalComps = list(_originalComps)

    with open(file_path, 'w') as fastaWriter:
        for orig_comp1, orig_comp2 in finalComp_to_pairs(_originalComps):
            read_sql = f"select * from reads where seq1_original_component = {orig_comp1} AND seq2_original_component = {orig_comp2}"
            read_curs = conn.execute(read_sql)
            rows = read_curs.fetchall()
            for row in rows:
                fastaWriter.write(f">{row[0]}.1\t{row[3]}\n{row[1]}\n")
                fastaWriter.write(f">{row[0]}.2\t{row[4]}\n{row[2]}\n")

    conn.close()


output_dir = f"dumped_partitions_cutoff{cutoff_threshold}_" + os.path.basename(sqlite_db_path).replace(".db", '')
os.makedirs(output_dir)

all_params = list()
# Debugging activated
for finalComp, originalComps in components.get_components_dict().items():
    file_name = os.path.join(output_dir, f"{finalComp}.fa")
    # Debugging
    if finalComp == 1:
        all_params.append((file_name, finalComp, originalComps))

with multiprocessing.Pool(no_cores) as pool:
    for _ in tqdm(pool.imap_unordered(perform_writing, all_params), total=len(all_params)):
        pass
