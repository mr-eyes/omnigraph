"""
This script is used under the project "Omnigraph"

Input:
    1. sqlite DB File
    2. PairsCount TSV file generated by the executable: single_primaryPartitioning.
    3. cutoff threshold, filter out edges with weight < cutoff.

Output:
    1. Directory of fasta files, each file contains a final component sequences.
    2. TSV with col1:finalCompID col2:originalComponentsIDs


Run:
python dump_finalComps.py <db_file> <pairsCountFile> <no_cores> <optional: cutoff (default: 1)>
"""

import sqlite3
import sys
from collections import defaultdict
import os
from tqdm import tqdm
import multiprocessing


class ConnectedComponents:

    def __init__(self, min_count=1):
        self.__source = list()
        self.__target = list()
        self.final_components = dict()
        self.number_of_components = int()
        self.THRESHOLD = min_count

    def add_edge(self, source_node, target_node, pair_count):
        if pair_count >= self.THRESHOLD:
            self.__source.append(source_node)
            self.__target.append(target_node)

    def construct(self):
        __leaders = defaultdict(lambda: None)
        __groups = defaultdict(set)

        def __find(x):
            l = __leaders[x]
            if l is not None:
                l = __find(l)
                __leaders[x] = l
                return l
            return x

        def __union(x, y):
            lx, ly = __find(x), __find(y)
            if lx != ly:
                __leaders[lx] = ly

        for i in range(len(self.__source)):
            __union(self.__source[i], self.__target[i])

        for x in __leaders:
            __groups[__find(x)].add(x)

        for component_id, (k, v) in enumerate(__groups.items(), start=1):
            self.final_components[component_id] = v

        self.number_of_components = len(self.final_components)

    def get_components_dict(self):
        return self.final_components

    def dump_to_tsv(self, file_name):
        with open(file_name, 'w') as tsvWriter:
            for compID, nodes in self.final_components.items():
                nodes = ','.join(map(str, nodes))
                tsvWriter.write(f"{compID}\t{nodes}\n")

    def __del__(self):
        del self


def get_nodes_sizes(components_file_path):
    """
    Return the size of each original component so we can apply threshold on weight.
    :param components_file_path:
    :return: node_to_size
    """
    node_to_size = dict()
    with open(components_file_path, 'r') as compsReader:
        for _line in compsReader:
            _line = list(map(int, _line.strip().split(',')))
            _compID = int(_line[0])
            node_to_size[_compID] = len(_line) - 1

    return node_to_size


if len(sys.argv) < 4:
    raise ValueError("run: python dump_finalComps.py dump_finalComps.py <db_file> <pairsCountFile> <no_cores> <cutoff>")

cutoff_threshold = 1
sqlite_db_path = sys.argv[1]
pairsCountFile = sys.argv[2]
no_cores = int(sys.argv[3])

if len(sys.argv) == 5:
    cutoff_threshold = int(sys.argv[4])

"""
1. Parse the pairsCount to edges
"""

edges = []
with open(pairsCountFile, 'r') as pairsCountReader:
    next(pairsCountReader)  # skip header
    for line in pairsCountReader:
        edges.append(tuple(map(int, line.strip().split())))

"""
2. Construct final components
"""

components = ConnectedComponents(min_count=cutoff_threshold)
for edge in edges:
    components.add_edge(*edge)

components.construct()
number_of_final_components = components.number_of_components

print(f"Number of connected comps: {number_of_final_components}")

"""
3. Multithreaded dumping the partitions to fasta files
"""


def perform_writing(params):
    file_path, _finalCompID, _originalComps = params
    conn = sqlite3.connect(sqlite_db_path)
    _originalComps = list(_originalComps)

    # Split into chunks of 100 originalComponent
    chunk_size = 100
    originalCompsChunks = [_originalComps[i * chunk_size:(i + 1) * chunk_size] for i in range((len(_originalComps) + chunk_size - 1) // chunk_size)]

    with open(file_path, 'w') as fastaWriter:

        # Do it in chunks
        for chunk_originalComp in originalCompsChunks:
            for i in range(len(chunk_originalComp)):
                for j in range(len(chunk_originalComp)):
                    read_sql = f"select * from reads where seq1_original_component = {chunk_originalComp[i]} AND seq1_original_component = {chunk_originalComp[j]}"
                    read_curs = conn.execute(read_sql)
                    for row in read_curs:
                        fastaWriter.write(f">{row[0]}.1\t{row[3]}\n{row[1]}\n")
                        fastaWriter.write(f">{row[0]}.2\t{row[4]}\n{row[2]}\n")

    conn.close()


# version 2

# def perform_writing(params):
#     file_path, _finalCompID, _originalComps = params
#     conn = sqlite3.connect(sqlite_db_path)
#     _originalComps = list(_originalComps)
#
#     # Split into chunks of 100 originalComponent
#     chunk_size = 100
#     originalCompsChunks = [_originalComps[i * chunk_size:(i + 1) * chunk_size] for i in range((len(_originalComps) + chunk_size - 1) // chunk_size)]
#
#     with open(file_path, 'w') as fastaWriter:
#
#         # Do it in chunks
#
#         for chunk_originalComp in originalCompsChunks:
#             chunk_originalComp = tuple(chunk_originalComp)
#
#             read_1_sql = "select * from reads where seq1_original_component in ({seq})".format(seq=','.join(['?'] * len(chunk_originalComp)))
#             read_1_curs = conn.execute(read_1_sql, chunk_originalComp)
#
#             read_2_sql = "select * from reads where seq2_original_component in ({seq})".format(seq=','.join(['?'] * len(chunk_originalComp)))
#             read_2_curs = conn.execute(read_2_sql, chunk_originalComp)
#
#             for row in read_1_curs:
#                 fastaWriter.write(f">{row[0]}.1\t{row[3]}\n{row[1]}\n")
#
#             for row in read_2_curs:
#                 fastaWriter.write(f">{row[0]}.2\t{row[4]}\n{row[2]}\n")
#
#     conn.close()


# Old version, related to issue #4
# def perform_writing(params):
#     file_path, _finalCompID, _originalComps = params
#     conn = sqlite3.connect(sqlite_db_path)
#     _originalComps = tuple(_originalComps)
#     read_1_sql = "select * from reads where seq1_original_component in ({seq})".format(seq=','.join(['?'] * len(_originalComps)))
#     read_1_curs = conn.execute(read_1_sql, _originalComps)
#
#     read_2_sql = "select * from reads where seq1_original_component in ({seq})".format(seq=','.join(['?'] * len(_originalComps)))
#     read_2_curs = conn.execute(read_2_sql, _originalComps)
#
#     with open(file_path, 'w') as fastaWriter:
#         for row in read_1_curs:
#             fastaWriter.write(f">{row[0]}.1\t{row[3]}\n{row[1]}\n")
#         for row in read_2_curs:
#             fastaWriter.write(f">{row[0]}.2\t{row[4]}\n{row[2]}\n")
#
#     conn.close()


output_dir = f"dumped_partitions_cutoff{cutoff_threshold}_" + os.path.basename(sqlite_db_path).replace(".db", '')
os.makedirs(output_dir)

all_params = list()
for finalComp, originalComps in components.get_components_dict().items():
    file_name = os.path.join(output_dir, f"{finalComp}.fa")
    all_params.append((file_name, finalComp, originalComps))

with multiprocessing.Pool(no_cores) as pool:
    for _ in tqdm(pool.imap_unordered(perform_writing, all_params), total=len(all_params)):
        pass
